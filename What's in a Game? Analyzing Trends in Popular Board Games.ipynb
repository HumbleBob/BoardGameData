{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's in a Game? Analyzing Trends in Popular Board Games\n",
    "### Robert Choi, Andrew Walter-McNeill"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  jQuery(document).ready(function($) {  \n",
    "  \n",
    "  $(window).on('load', function(){\n",
    "    $('#preloader').fadeOut('slow',function(){$(this).remove();});\n",
    "  });\n",
    "  \n",
    "  });\n",
    "</script>\n",
    "\n",
    "<style type=\"text/css\">\n",
    "  div#preloader { position: fixed; \n",
    "      left: 0; \n",
    "      top: 0; \n",
    "      z-index: 999; \n",
    "      width: 100%; \n",
    "      height: 100%; \n",
    "      overflow: visible; \n",
    "      background: #fff url('http://preloaders.net/preloaders/720/Moving%20line.gif') no-repeat center center; \n",
    "  }\n",
    "\n",
    "</style>\n",
    "\n",
    "<div id=\"preloader\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  } \n",
    "  \n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false; \n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "One of the reasons data mining is such a powerful way to analyze data is that there are so many different classification approaches to chose from. This versatility means that there are likely multiple viable classifiers that can be used to analyze a given data set, and thus multiple ways to address a given problem. However, in order to take full advantage of this versatility, it is necessary to decide which classification approach is best for each data set. In the following project, we explore the relative effectiveness of two classification approaches, decision trees and artificial neural nets, on a data set of popular board games.\n",
    "\n",
    "One of our group members is a board game hobbyist and was intrigued by the idea of investigating what sort of qualities, if any, tend to make a board game well received. Using decision trees and ANNs, we hope to analyze data collected from BoardGameGeek.com for these potential trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The raw dataset\n",
    "The dataset that we examined is a record of the BoardGameGeek.com “Board Game Rank” list compiled in March 2017. The list comprises 90,444 board games, about 5,000 of which are sufficiently well-known to be assigned a rating by the website’s hobbyist community—our dataset is a list of these 5,000 games. Each entry in the list includes the game’s name, publisher, author, date of publication, and “at-a-glance” information about the game’s content, such as its genre, mechanics, average play length, and degree of complexity. Our dataset can be used to investigate whether or not board games possessing particular qualities tend to be rated more highly by the website’s users, and it is remarkably robust in that it is vast, possesses no missing entries, and has perfect internal consistency (e.g. the mechanics classifications are assigned from a pre-made list, rather than assigned ad hoc). A full list of the attributes from the data that were considered by our learners is as follows:\n",
    "##### min_players \n",
    "The minimum number of players required to play a game. Typically 1-12, strictly integers. In our experience, 2-4 player games tend to rank most highly.\n",
    "##### max_players \n",
    "The maximum number of players allowed to play a game. Typically 2-12, strictly integers.\n",
    "##### avg_time\n",
    "The average number of minutes required to complete a game. Typically 15-240, strictly integers.\n",
    "##### min_time\n",
    "If, on a game's box, a range of times is printed for game length, then this attribute is the low end of that range. Typically 15-180, strictly integers.\n",
    "##### max_time\n",
    "If, on a game's box, a range of times is printed for game length, then this attribute is the high end of the range. Typically 30-240, strictly integers.\n",
    "##### age\n",
    "The guideline age printed on a game's box that recommends the minimum age of a player that can be expected to enjoy play. Typically 8-15, strictly integers.\n",
    "##### mechanic\n",
    "A list of categorical values describing the sorts of game mechanicsthat are included in a game's play. Examples include, \"Trading\", \"Action Point Allowance System\", \"Co-operative Play\", etc.\n",
    "##### weight\n",
    "A decimal value on a 5-point scale indicating how complex a game is, based on a number of factors, such as length of the game, length of time it takes to learn the game, complexity of the rule book, simpicity of design, length of time spent planning turn actions, and the amount of luck involved in play. Values range from 1-5, where 1 is \"light\" and 5 is \"heavy\".\n",
    "##### geek_rating\n",
    "The average of all user submitted ratings for a game with Bayesian Averaging. That is, the average, with additional dummy votes factored in. The effect of Bayesian Averaging is to favor games that have received many user votes over games with fewer user votes; generally, a game's rating will become more accurate as more votes accrue. This is our main class attribute, used to represent a game’s overall quality. Possible ratings are from 1-10, but, in practice, values range from 5-9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "The raw dataset has two main problems with it: it contains many categorical attributes, and its class attribute is continuous. There were no missing values. In order to deal with the first issue, we selected the categorical attribute we felt contained data most relevant to our class attribute (mechanic) and turned it into 52 binary attributes. We then removed the other categorical attributes. To address the second issue, we turned the continuous class attribute, geek rank, into five quantiles. This left us with 4999 examples, 61 attributes, and one class attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "Decision trees work by separating a data set into different nested “nodes” of data with the same value for a given attribute. They are a simple, elegant way of dealing with samples with many attributes, are robust to noisy data, can be easily understood by humans, and usually yield results that are appreciably better than random guessing. Artificial neural nets work by passing the attribute values of a given sample through a series of nodes until a single output signal is produced, much in the same way as neurons pass signals along in the brain. While ANNs are much more difficult for humans to understand than decision trees, they are uniquely well equipped to handle Big Data (incremental training, dynamic data sets etc) and are thus an increasingly intriguing option. They are also robust to noise and can easily deal with complex attributes and data sets.\n",
    "\n",
    "For our five decision tree runs, we chose to manipulate the criterion used to make splits, the maximum depth of the tree, the minimum samples required to form a leaf, and the maximum number of leaf nodes. The logic for each manipulation is as follows (parameters listed):\n",
    "##### v1: (criterion = entropy)\n",
    "The Control model\n",
    "##### v2: (criterion = gini)\n",
    "The ‘entropy’ algorithm utilizes information gain to make splits .  Information gain is a measure of exactly what it sounds like: the information gained from a particular split. A 50/50 split divides the data no better than random distribution, and thus provides the least amount of information possible, whereas a 100/0 split provides the most information possible (if a sample has that combination of attribute values, its class value is known). The ‘entropy’ algorithm calculates the potential information gain for all possible splits, and chose the split that results in the highest IG. The ‘gini’ algorithm operates by determining what the probability of a improper classification is for each sample in a given node if said classification was done randomly within the proportions of attribute value distribution in the given node. This, combined with the control is an exploration of the relative effectiveness of the two splitting algorithms.\n",
    "##### v3: (criterion = ‘entropy’, max_depth = 2)\n",
    "One of the ways to combat overfitting in decision trees is to limit the depth of the tree. By limiting how many layers down a tree can build, one also limits the complexity of the tree, and thus lowers the likelihood that it will conform too specifically to a given data set. We chose a value of 2 because 2 is the most restrictive possible setting; by making the change to the classifier as dramatic as possible, we hope to most clearly illuminate any trends that might result from analogous changes. This is perhaps the most basic approach to combatting overfitting, as it does not take into account more informative measures of complexity, like the size of the nodes in the tree or the number of nodes in the tree.\n",
    "##### v4: (criterion = ‘entropy, min_samples_leaf = 25)\n",
    "Another way to combat overfitting in decision trees is to define a minimum number of samples each leaf must have. This constraint, not unlike the max_depth constraint, limits the complexity of the decision tree by forcing it to stop splitting nodes at an earlier point than it otherwise would have (resulting in less overall nodes). After brief visual analysis of the unconstrained decision tree, we noticed that most of the smaller leaves contained around 10 or 15 examples. We thought doubling this number would give us an appreciable reduction in complexity without sacrificing too much accuracy, and so chose 25.\n",
    "##### v5: (criterion = ‘entropy’, max_leaf_nodes = 50)\n",
    "One more way to combat overfitting is to limit the number of leaves the tree can have. This essentially accomplishes the same thing as the previous two parameter changes, limiting the complexity of the tree, but it does so in a different way. After brief visual analysis, we noticed that the unconstrained tree had a little less than 100 leaves. So as to be consistent with our factor-of-two approach in the last classifier and hopefully accomplish the same thing, we decided to half the unconstrained value and got 50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "6.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
