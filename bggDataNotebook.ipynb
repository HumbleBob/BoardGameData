{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's in a Game? Analyzing Trends in Popular Board Games\n",
    "### Robert Choi, Andrew Walter-McNeill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "One of the reasons data mining is such a powerful way to analyze data is that there are so many different classification approaches to chose from. This versatility means that there are likely multiple viable classifiers that can be used to analyze a given data set, and thus multiple ways to address a given problem. However, in order to take full advantage of this versatility, it is necessary to decide which classification approach is best for each data set. In the following project, we explore the relative effectiveness of two classification approaches, decision trees and artificial neural nets, on a data set of popular board games.\n",
    "\n",
    "One of our group members is a board game hobbyist and was intrigued by the idea of investigating what sort of qualities, if any, tend to make a board game well received. Using decision trees and ANNs, we hope to analyze data collected from BoardGameGeek.com for these potential trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we examined is a record of the BoardGameGeek.com “Board Game Rank” list compiled in March 2017. The list comprises 90,444 board games, about 5,000 of which are sufficiently well-known to be assigned a rating by the website’s hobbyist community—our dataset is a list of these 5,000 games. Each entry in the list includes the game’s name, publisher, author, date of publication, and “at-a-glance” information about the game’s content, such as its genre, mechanics, average play length, and degree of complexity. Our dataset can be used to investigate whether or not board games possessing particular qualities tend to be rated more highly by the website’s users, and it is remarkably robust in that it is vast, possesses no missing entries, and has perfect internal consistency (e.g. the mechanics classifications are assigned from a pre-made list, rather than assigned ad hoc). A full list of the attributes from the data that were considered by our learners is as follows:\n",
    "##### min_players \n",
    "The minimum number of players required to play a game. Typically 1-12, strictly integers. In our experience, 2-4 player games tend to rank most highly.\n",
    "##### max_players \n",
    "The maximum number of players allowed to play a game. Typically 2-12, strictly integers.\n",
    "##### avg_time\n",
    "The average number of minutes required to complete a game. Typically 15-240, strictly integers.\n",
    "##### min_time\n",
    "If, on a game's box, a range of times is printed for game length, then this attribute is the low end of that range. Typically 15-180, strictly integers.\n",
    "##### max_time\n",
    "If, on a game's box, a range of times is printed for game length, then this attribute is the high end of the range. Typically 30-240, strictly integers.\n",
    "##### age\n",
    "The guideline age printed on a game's box that recommends the minimum age of a player that can be expected to enjoy play. Typically 8-15, strictly integers.\n",
    "##### mechanic\n",
    "A list of categorical values describing the sorts of game mechanicsthat are included in a game's play. Examples include, \"Trading\", \"Action Point Allowance System\", \"Co-operative Play\", etc.\n",
    "##### weight\n",
    "A decimal value on a 5-point scale indicating how complex a game is, based on a number of factors, such as length of the game, length of time it takes to learn the game, complexity of the rule book, simpicity of design, length of time spent planning turn actions, and the amount of luck involved in play. Values range from 1-5, where 1 is \"light\" and 5 is \"heavy\".\n",
    "##### geek_rating\n",
    "The average of all user submitted ratings for a game with Bayesian Averaging. That is, the average, with additional dummy votes factored in. The effect of Bayesian Averaging is to favor games that have received many user votes over games with fewer user votes; generally, a game's rating will become more accurate as more votes accrue. This is our main class attribute, used to represent a game’s overall quality. Possible ratings are from 1-10, but, in practice, values range from 5-9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "The raw dataset has two main problems with it: it contains many categorical attributes, and its class attribute is continuous. There were no missing values. In order to deal with the first issue, we selected the categorical attribute we felt contained data most relevant to our class attribute (mechanic) and turned it into 52 binary attributes. We then removed the other categorical attributes. To address the second issue, we turned the continuous class attribute, geek rank, into five quantiles. This left us with 4999 examples, 61 attributes, and one class attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     26,
     117,
     143,
     158,
     174,
     184,
     205
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree                               # to do DTs\n",
    "from sklearn.datasets import fetch_mldata              # to import data\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPclassifier       # MLP: multi-layer perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#in order to directly edit individual cells in a pandas dataframe, this command\n",
    "#must be issued\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/HumbleBob/BoardGameData/master/original_data.csv'\n",
    "\n",
    "#genfromtxt reads data into a ndarray, chose this because loadtxt was not \n",
    "#working. Forces type string for each element so that all data can be \n",
    "#interpereted (specifying none defaults to ASCII byte codes because of the\n",
    "#strange encoding of the file). Selects important columns\n",
    "\n",
    "def transformData(raw_data):\n",
    "\n",
    "    cols = raw_data[0]\n",
    "    numpy_data = raw_data[1:]\n",
    "\n",
    "\n",
    "    #CREATES DATAFRAME\n",
    "    #transforms ndarray into pandas dataframe with headers specified by the first\n",
    "    #row\n",
    "    data = pd.DataFrame(data=numpy_data, columns=cols)\n",
    "    #specifies which attributes should be numeric (as they were all read in as str)\n",
    "    numeric_columns = ['min_players', 'max_players', 'avg_time', 'min_time', \n",
    "                       'max_time', 'geek_rating', 'num_votes', 'age', 'weight']\n",
    "    #translates specified columns into floats\n",
    "    data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric)\n",
    "\n",
    "    #creates quantiles for class attribute\n",
    "    geek_quantiles = pd.qcut(data['geek_rating'], 5, labels=False)\n",
    "    data['geek_quantiles'] = geek_quantiles\n",
    "\n",
    "\n",
    "    #PROCESSES CATEGORICAL ATTRIBUTE\n",
    "    #first creates empty list to store all categories\n",
    "    mechanics = []\n",
    "    #iterates over all attribute values (currently in str form)\n",
    "    for string_mechs in data['mechanic']:\n",
    "        #turns str into list of mechanics (find/replaced commas w '*' in excel)\n",
    "        list_mechs = string_mechs.split('*')\n",
    "        #gets rid of superfluous spaces and adds mechanic to master list if not\n",
    "        #already there\n",
    "        for mech in list_mechs:\n",
    "            if mech[0] == ' ':\n",
    "                clean_mech = mech[1:]\n",
    "                if clean_mech not in mechanics:\n",
    "                    mechanics.append(clean_mech)\n",
    "            elif mech not in mechanics:\n",
    "                mechanics.append(mech)\n",
    "\n",
    "    #creates new attribute to store each example's mechanics in list form\n",
    "    data['mechanic_list'] = 0\n",
    "\n",
    "    #iterates over index of DataFrame\n",
    "    for index in range(4999):\n",
    "        #gets rid of superfluous spaces again\n",
    "        list_mechs = []\n",
    "        dirty_mechs = data['mechanic'][index].split('*')\n",
    "        for mech in dirty_mechs:\n",
    "            if mech[0] == ' ':\n",
    "                clean_mech = mech[1:]\n",
    "                list_mechs.append(clean_mech)\n",
    "            else: \n",
    "                list_mechs.append(mech)\n",
    "        #for each example, defines newly created 'mechanic list' attribute to be\n",
    "        #a list of its mechanics\n",
    "        data['mechanic_list'][index] = list_mechs\n",
    "\n",
    "    #creates 43 new binary attributes, one for each mechanic\n",
    "    for mechanic in mechanics:\n",
    "        data[mechanic] = 0\n",
    "\n",
    "    #defines each binary appropriately: if the attribute appears in the an example's\n",
    "    #list of mechanics, defined as 1. Otherwise, 0.\n",
    "    for index in range(4999):\n",
    "        for mechanic in mechanics:\n",
    "            if mechanic in data['mechanic_list'][index]:\n",
    "                data[mechanic][index] = 1\n",
    "\n",
    "    #removes unneccessary attributes\n",
    "    del data['mechanic_list']\n",
    "    del data['mechanic']\n",
    "    del data['geek_rating']\n",
    "    del data['num_votes']\n",
    "    \n",
    "    #data = data.iloc[:,1:]\n",
    "    \n",
    "    #removes the headers from the rest of the data\n",
    "    headers = data.iloc[0,:]\n",
    "    #str_data = data.iloc[1:,:]\n",
    "    #turns the data from floats into strings\n",
    "    data = data.astype(float)\n",
    "    #separates the data from the class attributes\n",
    "    X = data.iloc[:,np.arange(len(headers)) != 7]\n",
    "    y = data.iloc[:,7]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "raw_data = np.genfromtxt(url, delimiter = ',', dtype=str, \n",
    "                        usecols=(3,4,5,6,7,10,11,13,14,17))\n",
    "X, y = transformData(raw_data)\n",
    "\n",
    "\n",
    "def confusionMatrix(X, y, learning, splitSize=.2, lbls=None):\n",
    "    '''Create a confusion matrix and report the matrix and the score\n",
    "    for one training/test split, given X, y, and a learning model as \n",
    "    an input. Allow the size of the split to be given as a parameter as well.\n",
    "    '''\n",
    "    # Make one test training split\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,\n",
    "                                                    test_size=splitSize,\n",
    "                                                    random_state=10)\n",
    "    model = learning.fit(Xtrain, ytrain)\n",
    "    y_model = model.predict(Xtest)\n",
    "    \n",
    "    score = accuracy_score(ytest, y_model)\n",
    "    # if specific labels are specified, implement those labels in the\n",
    "    # creation of the confusion matrix; otherwise, don't bother.\n",
    "    if(lbls==None):\n",
    "        mat = confusion_matrix(ytest, y_model)\n",
    "    else:\n",
    "        mat = confusion_matrix(ytest,\n",
    "                               y_model,\n",
    "                               labels=lbls)\n",
    "    print(\"Score and Confusion Matrix to illustrate problem complexity\")\n",
    "    print(\"Score: \", score)\n",
    "    print(\"Confusion Matrix:\\n\", mat)\n",
    "    print()\n",
    "\n",
    "def fiveFoldCVs(X, y, models):\n",
    "    '''Performs five five fold cross validations given X, y, and a \n",
    "    learning model\n",
    "    '''\n",
    "    cvs = []\n",
    "    \n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # For each model passed to the function, perform a 5foldcv and\n",
    "    # add it to the array to be returned\n",
    "    for i in range(len(models)):\n",
    "        score = cross_val_score(models[i], X, y, cv=cv)\n",
    "        cvs.append(score)\n",
    "    return cvs\n",
    "\n",
    "def getPValues(cvs):\n",
    "    '''Get the p-values for each pair of models, having been passed the \n",
    "    cv results\n",
    "    '''\n",
    "    # Basically, this offset variable and for loops exist so that every pair\n",
    "    # of models will have its p-value computed with no repeats\n",
    "    offset = 0\n",
    "    for i in range(len(cvs)):\n",
    "        for j in range(offset, len(cvs)):\n",
    "            if (i != j):\n",
    "                # p-values are computed and printed here\n",
    "                pair = stats.ttest_rel(cvs[i], cvs[j])\n",
    "                reportStr = \"The p-value for \" + str(i) + \",\" + str(j) + \" is \" + str(pair[1]) + \"\\n\"\n",
    "                print(reportStr)\n",
    "        offset += 1\n",
    "\n",
    "def getAverages(a):\n",
    "    '''get the averages of the input, an array of arrays (result of \n",
    "    cross validations)\n",
    "    '''\n",
    "    result = []\n",
    "    for i in a:\n",
    "        result.append(i.mean())\n",
    "    # return an array of averages corresponding to each array in the input\n",
    "    return result\n",
    "\n",
    "def getIntervals(a):\n",
    "    '''get the confidence intervals for an array of arrays \n",
    "    (result of cross validations)\n",
    "    '''\n",
    "    result = []\n",
    "    # set this mysterious value\n",
    "    z_critical = stats.norm.ppf(q = 0.975)\n",
    "    # get the averages to work with\n",
    "    means = getAverages(a)\n",
    "\n",
    "    # for each array\n",
    "    for i in range(len(a)):\n",
    "        # determine the standard deviation\n",
    "        standard_deviation = a[i].std()\n",
    "        # to calculate the margin of error\n",
    "        margin_error = (z_critical * standard_deviation) / (math.sqrt(len(a[i])))\n",
    "        \n",
    "        # so as to calculate the confidence interval and add it to the result\n",
    "        result.append([means[i] - margin_error, means[i] + margin_error])\n",
    "    return result\n",
    "\n",
    "def cIPlot(data, ymin=0.3):\n",
    "    '''Create a confidence interval graph and save it as a file. The\n",
    "    results of the cross validations are passed as a parameter, as is\n",
    "    the name of the file to be saved.\n",
    "    '''\n",
    "    # Get the averages and confidence intervals for the graph\n",
    "    sample_means = getAverages(data)\n",
    "    intervals = getIntervals(data)\n",
    "\n",
    "    # Set size\n",
    "    plt.figure(figsize=(9,9))\n",
    "\n",
    "    xvals = np.arange(5, 30, 5) # This is important given the size of the data\n",
    "                                # somehow.\n",
    "    yerrors = [(top-bot)/2 for top,bot in intervals]\n",
    "\n",
    "    plt.errorbar(x=xvals,\n",
    "                 y=sample_means,\n",
    "                 yerr=yerrors,\n",
    "                 fmt='D')\n",
    "\n",
    "    # Standardize the graph scale\n",
    "    plt.axis(xmin=0,xmax=30,ymin=ymin,ymax=1.05)\n",
    "    # Apply labels\n",
    "    labels = [\"v1\", \"v2\", \"v3\", \"v4\", \"v5\"]\n",
    "    plt.xticks(xvals,labels)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "Decision trees work by separating a data set into different nested “nodes” of data with the same value for a given attribute. They are a simple, elegant way of dealing with samples with many attributes, are robust to noisy data, can be easily understood by humans, and usually yield results that are appreciably better than random guessing. Artificial neural nets work by passing the attribute values of a given sample through a series of nodes until a single output signal is produced, much in the same way as neurons pass signals along in the brain. While ANNs are much more difficult for humans to understand than decision trees, they are uniquely well equipped to handle Big Data (incremental training, dynamic data sets etc) and are thus an increasingly intriguing option. They are also robust to noise and can easily deal with complex attributes and data sets.\n",
    "\n",
    "For our five decision tree runs, we chose to manipulate the criterion used to make splits, the maximum depth of the tree, the minimum samples required to form a leaf, and the maximum number of leaf nodes. The logic for each manipulation is as follows (parameters listed):\n",
    "##### v1: (criterion = entropy)\n",
    "The Control model\n",
    "##### v2: (criterion = gini)\n",
    "The ‘entropy’ algorithm utilizes information gain to make splits .  Information gain is a measure of exactly what it sounds like: the information gained from a particular split. A 50/50 split divides the data no better than random distribution, and thus provides the least amount of information possible, whereas a 100/0 split provides the most information possible (if a sample has that combination of attribute values, its class value is known). The ‘entropy’ algorithm calculates the potential information gain for all possible splits, and chose the split that results in the highest IG. The ‘gini’ algorithm operates by determining what the probability of a improper classification is for each sample in a given node if said classification was done randomly within the proportions of attribute value distribution in the given node. This, combined with the control is an exploration of the relative effectiveness of the two splitting algorithms.\n",
    "##### v3: (criterion = ‘entropy’, max_depth = 2)\n",
    "One of the ways to combat overfitting in decision trees is to limit the depth of the tree. By limiting how many layers down a tree can build, one also limits the complexity of the tree, and thus lowers the likelihood that it will conform too specifically to a given data set. We chose a value of 2 because 2 is the most restrictive possible setting; by making the change to the classifier as dramatic as possible, we hope to most clearly illuminate any trends that might result from analogous changes. This is perhaps the most basic approach to combatting overfitting, as it does not take into account more informative measures of complexity, like the size of the nodes in the tree or the number of nodes in the tree.\n",
    "##### v4: (criterion = ‘entropy, min_samples_leaf = 25)\n",
    "Another way to combat overfitting in decision trees is to define a minimum number of samples each leaf must have. This constraint, not unlike the max_depth constraint, limits the complexity of the decision tree by forcing it to stop splitting nodes at an earlier point than it otherwise would have (resulting in less overall nodes). After brief visual analysis of the unconstrained decision tree, we noticed that most of the smaller leaves contained around 10 or 15 examples. We thought doubling this number would give us an appreciable reduction in complexity without sacrificing too much accuracy, and so chose 25.\n",
    "##### v5: (criterion = ‘entropy’, max_leaf_nodes = 50)\n",
    "One more way to combat overfitting is to limit the number of leaves the tree can have. This essentially accomplishes the same thing as the previous two parameter changes, limiting the complexity of the tree, but it does so in a different way. After brief visual analysis, we noticed that the unconstrained tree had a little less than 100 leaves. So as to be consistent with our factor-of-two approach in the last classifier and hopefully accomplish the same thing, we decided to half the unconstrained value and got 50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Trees\n",
      "\n",
      "Score and Confusion Matrix to illustrate problem complexity\n",
      "Score:  0.259\n",
      "Confusion Matrix:\n",
      " [[53 39 56 40 19]\n",
      " [50 34 46 44 20]\n",
      " [45 34 48 32 37]\n",
      " [41 26 45 52 38]\n",
      " [16 33 38 42 72]]\n",
      "\n",
      "Confidence Intervals:\n",
      "v0:  [ 0.267  0.242  0.265  0.249  0.255]\n",
      "v1:  [ 0.263  0.259  0.256  0.262  0.252]\n",
      "v2:  [ 0.251  0.254  0.255  0.256  0.262]\n",
      "v3:  [ 0.279  0.28   0.278  0.277  0.275]\n",
      "v4:  [ 0.275  0.285  0.293  0.275  0.283]\n",
      "\n",
      "The p-value for 0,1 is 0.613708914439\n",
      "\n",
      "The p-value for 0,2 is 1.0\n",
      "\n",
      "The p-value for 0,3 is 0.0104434683189\n",
      "\n",
      "The p-value for 0,4 is 0.00877176667408\n",
      "\n",
      "The p-value for 1,2 is 0.486049910088\n",
      "\n",
      "The p-value for 1,3 is 0.000286099233405\n",
      "\n",
      "The p-value for 1,4 is 0.00849761608976\n",
      "\n",
      "The p-value for 2,3 is 0.00102675240209\n",
      "\n",
      "The p-value for 2,4 is 0.00161076662681\n",
      "\n",
      "The p-value for 3,4 is 0.270487852341\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAIMCAYAAABVH87kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fe455f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_decision_trees(X, y):\n",
    "    '''Function to run all parts of DT creation and testing\n",
    "    '''\n",
    "    print(\"Decision Trees\\n\")\n",
    "\n",
    "    # Make a list of trees\n",
    "    trees = []\n",
    "    #creates 5 different decision tree classifiers with different parameters and \n",
    "    #fits the data to them. See paper for parameter explanations\n",
    "    dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "    trees.append(dt)\n",
    "    dt = tree.DecisionTreeClassifier(criterion='gini')\n",
    "    trees.append(dt)\n",
    "    dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth = 2)\n",
    "    trees.append(dt)\n",
    "    dt = tree.DecisionTreeClassifier(criterion='entropy', min_samples_leaf = 25)\n",
    "    trees.append(dt)\n",
    "    dt = tree.DecisionTreeClassifier(criterion='entropy', max_leaf_nodes = 50)\n",
    "    trees.append(dt)\n",
    "    \n",
    "    return trees\n",
    "\n",
    "trees = create_decision_trees(X, y)\n",
    "\n",
    "confusionMatrix(X, y, trees[0])\n",
    "\n",
    "# Compute cross validations for each model and print them\n",
    "cvs = fiveFoldCVs(X, y, trees)\n",
    "print(\"Confidence Intervals:\")\n",
    "for i in range(len(cvs)):\n",
    "    print(\"v\" + str(i) + \": \", cvs[i])\n",
    "print()\n",
    "\n",
    "# Compute p-values\n",
    "getPValues(cvs)\n",
    "\n",
    "# Create file for confidence intervals\n",
    "cIPlot(cvs, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANNs\n",
    "After researching the neural net parameters, learnrate stood out to us as the most intriguing. We thus focused our trials on manipulating learnrate, providing two controls. We also explored more basic changes to the number of iterations and the structure of the neural net. The logic for each manipulation is as follows (parameters listed):\n",
    "##### v1: (X, y, numIter=500)\n",
    "Control. Default number of iterations has been raised to increase general\n",
    "accuracy.\n",
    "##### v2: (X, y, numIter=500, numNodes = 15, numLayers = 3)\n",
    "For this manipulation, we chose to set the number of nodes to 15 and the number of hidden layers to 3. For most problems, a single hidden layer is sufficient, as the ANN weighting algorithms are complex enough that they do not need much to work with (see following versions), but we wanted to see what would happen when we set the number of layers at three, working with just the default other parameters. Theoretically, the increased complexity might increase accuracy, but it will also increase runtime.\n",
    "##### v3: (X, y, numIter=500, solve = ‘sgd’)\n",
    "Control (solve method). The default solver is ‘adam,’ which, according to\n",
    "sklearn’s website, is “works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score.” From brief research on stack exchange, the ‘adam’ solver is actually an adaptation of the more universal stochastic gradient descent (sgd); therefore, we thought it would be interesting to compare sklearn’s default solver to the primary default solver in the field. This exploration also provides the control for our investigations of learnrate, which all require the sgd solve method.\n",
    "##### v4: (X, y, numIter=500, learnRate = 'adaptive', solve = 'sgd')\n",
    "The learnrate of an artificial neural net determines how quickly the ANN deserts old generalizations of the dataset for new ones. For example, suppose the ANN encounters 10 board game samples that have average playtimes of 60 minutes and have a geek rating of 5 (the highest quantile). Then suppose it encountered an 11th board game that has an average playtime of 30 minutes, but also had a geek rating of 5. If the learnrate of the ANN is relatively high, it will assume that sometimes even games with playtimes lower than 60 minutes can be rated highly; if the learnrate is relatively low, it will assume that the 11th board game is an outlier, and that only board games with playtimes of 60 minutes can be rated highly. The ‘adaptive’ parameter keeps the learnrate of the ANN at a predetermined value throughout the course of training, unless the training loss stops decreasing. If training loss ever does begin to increase, the learning rate is divided by five. The ANN should get better and better at predicting as it trains, as it learns incrementally. If this does not happen, its training loss will increase, and it will begin to “learn faster.” Since a higher learning rate means longer train time, this is one way to find an reasonable learning rate for a given data set.\n",
    "##### v5: (X, y, numIter=500, learnRate = 'invscaling', solve = 'sgd')\n",
    "One other possible learnrate parameter is invscaling. Invscaling works by steadily\n",
    "decreasing the learning rate (roughly following exponential decay) over a time interval t. Because invscaling allows the ANN to learn very quickly initially and then more slowly as time goes on, it can provide runtime advantages without compromising accuracy  but only with static datasets . If the dataset it dynamic, and new examples are being constantly added, invscaling is a poor choice because it will not allow the ANN to update any of its initial beliefs as quickly as it should. Since our dataset is static, we should be able to appreciate an increase in runtime without a loss of accuracy by utilizing invscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0,
     19
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNs\n",
      "\n",
      "Score and Confusion Matrix to illustrate problem complexity\n",
      "Score:  0.309\n",
      "Confusion Matrix:\n",
      " [[ 94  17  21  18  57]\n",
      " [ 77  10  18  23  66]\n",
      " [ 61  13  23  18  81]\n",
      " [ 46  12  14  22 108]\n",
      " [ 14   1  10  16 160]]\n",
      "\n",
      "Confidence Intervals:\n",
      "v0:  [ 0.294  0.28   0.313  0.318  0.317]\n",
      "v1:  [ 0.309  0.277  0.32   0.311  0.306]\n",
      "v2:  [ 0.204  0.193  0.187  0.18   0.184]\n",
      "v3:  [ 0.204  0.193  0.187  0.182  0.184]\n",
      "v4:  [ 0.204  0.193  0.187  0.22   0.221]\n",
      "\n",
      "The p-value for 0,1 is 0.968486746913\n",
      "\n",
      "The p-value for 0,2 is 0.000462134240533\n",
      "\n",
      "The p-value for 0,3 is 0.000434831908457\n",
      "\n",
      "The p-value for 0,4 is 0.000138037239823\n",
      "\n",
      "The p-value for 1,2 is 0.000234853420856\n",
      "\n",
      "The p-value for 1,3 is 0.000221591583612\n",
      "\n",
      "The p-value for 1,4 is 0.000404695283501\n",
      "\n",
      "The p-value for 2,3 is 0.3739009663\n",
      "\n",
      "The p-value for 2,4 is 0.178239554897\n",
      "\n",
      "The p-value for 3,4 is 0.177858401051\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAIMCAYAAABVH87kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1060cba90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def makeMLP(X, y, numNodes=10, numLayers=3, numIter=200, learnRate='constant', solve='adam'):\n",
    "    '''Make an ANN. Allow the number of nodes and the number of hidden layers\n",
    "    and the maximum number of iterations to be set as parameters. \n",
    "    Returns the ANN.'''\n",
    "    \n",
    "    # Create a hidden layer tuple based on the parameters passed to the\n",
    "    # function\n",
    "    hiddenLayers = [numNodes for i in range(numLayers)]\n",
    "    hiddenLayers = tuple(hiddenLayers)\n",
    "\n",
    "    # Initialize ANN classifier using hiddenLayers tuple to set sizes\n",
    "    ann = MLPClassifier(hidden_layer_sizes=hiddenLayers, \n",
    "                        activation='logistic',      # default is 'relu'\n",
    "                        max_iter = numIter,         # default is 200\n",
    "                        learning_rate = learnRate,\n",
    "                        solver = solve,\n",
    "                        random_state=0)             # seed\n",
    "    return ann\n",
    "\n",
    "def create_anns(X, y):\n",
    "    print(\"ANNs\\n\")\n",
    "    \n",
    "    # Make a list of ANNs, each with a number of hidden nodes and number\n",
    "    # of iterations specified in the lists \"hNodes\" and \"maxIters\" respectively\n",
    "    # passed as a parameters\n",
    "    anns = []\n",
    "\n",
    "    #creates 5 artificial neural net models with 5 different sets of parameters. \n",
    "    #see paper for parameter explanations.\n",
    "    ann = makeMLP(X, y, numIter = 500)\n",
    "    anns.append(ann)\n",
    "    ann = makeMLP(X, y, numIter = 500, numNodes = 15, numLayers = 3)\n",
    "    anns.append(ann)\n",
    "    ann = makeMLP(X, y, numIter = 500, solve = 'sgd')\n",
    "    anns.append(ann)\n",
    "    ann = makeMLP(X, y, numIter = 500, learnRate = 'adaptive', solve = 'sgd')\n",
    "    anns.append(ann)\n",
    "    ann = makeMLP(X, y, numIter = 500, learnRate = 'invscaling', solve = 'sgd')\n",
    "    anns.append(ann)\n",
    "    \n",
    "    return anns\n",
    "\n",
    "anns = create_anns(X, y)\n",
    "\n",
    "confusionMatrix(X, y, anns[0])\n",
    "\n",
    "# Compute cross validations for each model and print them\n",
    "cvs = fiveFoldCVs(X, y, anns)\n",
    "print(\"Confidence Intervals:\")\n",
    "for i in range(len(cvs)):\n",
    "    print(\"v\" + str(i) + \": \", cvs[i])\n",
    "print()\n",
    "\n",
    "# Compute p-values\n",
    "getPValues(cvs)\n",
    "\n",
    "# Create file for confidence intervals\n",
    "cIPlot(cvs, 0.1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The maximum classification accuracy for all of our models and cross validations was 0.315, from v1 of our ANNs. The minimum classification accuracy was 0.18 from v3 of our ANNs. All of our classifiers have accuracies falling between this range of 0.18-0.315. Given the p-values for our decision trees, the differences between v1, v2, and v3 were not statistically significant, and the differences between v4 and v5 were also not statistically significant. However, the differences between these two groups of trees were statistically significant. Similarly, given the p-values for our ANNs, the differences between v1 and v2 were not statistically significant, and the differences between v3, v4, and v5 were also not statistically significant. However, the differences between these two groups of ANNs were statistically significant.\n",
    "## Observations\n",
    "None of our classifiers proved to be able to reliably classify games into the correct quantile based on the information provided to them. To be fair, the confusion matrices of both the decision trees and the ANNs show that, at the very least, the classifiers tended to be able to classify a game into a quantile close to their correct quantile. You can see this in that the values in the matrices are highest when they are closer to the central diagonal of the matrix, which represents correct classifications. This seems to indicate that somewhat good guesses can be made about a game’s overall quality based on information about its mechanics, complexity, number of players, and play time. However, given that the classifiers could not come to reliable conclusions about game quality, there are obviously other factors at play—one game with a given set of mechanics is not necessarily equal to another. Overall, among our decision trees, v4 and v5 performed the best, and their superior performance compared to v1, v2, and v3 is statistically significant. This suggests that the parameters implemented here to reduce\n",
    " overfitting were somewhat effective. Among our ANNs, v1 and v2 performed the best, and their superior performance compared to v3, v4, and v5 is statistically significant. This suggests that the default solver for sklearn’s MLPclassifier is superior compared to sgd.\n",
    "\n",
    "The ANNs tended to classify games slightly better than the decision trees. However, ANNs being what they are, it’s difficult to parse them for information about trends in game quality. By examining leaves in the decision tree with the best performance (v5), we have attempted to arrive at some rough conclusions about general trends in game quality.\n",
    "![title](https://raw.githubusercontent.com/HumbleBob/BoardGameData/master/choi.walterm.dt5.png)\n",
    "\n",
    "The first split of the tree is “Area-Impulse<=0.5”. Not many games have the Area-Impulse mechanic, but most of the games that do are in the lower quantiles; the decision tree likely makes this the first split because it allows it to deal with a much smaller number of games on one side of the tree, where most of them can be classified relatively easily as of lower quality.\n",
    "\n",
    "Most of the leaves where the geek_quantile values are predominantly low occur after an early split in the tree where the information gain split is “weight <= 2.6719”. This suggests that the learner identified that games in the lower percentiles of complexity often tend to be of lower quality.\n",
    "\n",
    "The path to one leaf where 83% of games are in the 4th and 5th quantile is as follows: no Area-Impulse mechanic, weight greater than 2.6719, cooperative play mechanic, average play time greater than 97.5 minutes. Greater than 97.5 minutes average play time is typical only of games with a high degree of complexity. Furthermore, because the Area-Impulse mechanic is fairly rare, this split also does not eliminate very many games. This path suggests that games with a cooperative play mechanic, above average complexity, and average play time tend to score quite highly. If you follow the split to games with fewer than 97.5 minutes average play time, the next split is determined by whether or not the recommended age is less than 4.\n",
    " Games where the recommended age is less than 4 tend to be bad, whereas games where the recommended age is greater than 4 tend to be of high quality. All of this suggests that the learner perceived a strong correlation between cooperative play and high game quality.\n",
    "\n",
    "One path in the tree, after following the “weight <= 2.6719” split to games with weight greater than 2.6719, consistently leads to leaves consisting primarily of games in the high quantiles. Along this path, leaves branch off from the path after being checked for particular mechanics. The first of these is point-to-point movement. The second is grid movement. The third is campaign / battle card driven. This path suggests that non-cooperative games of above average complexity, with game lengths above 42.5 minutes, so long as they possess at least one of these mechanics, tend to be of high quality.\n",
    "\n",
    "Further trends could be ascertained by analyzing the paths of the decision tree further.\n",
    "## Next Steps\n",
    "Our two learning approaches with various parameters failed to create a reliable classifier for predicting the quality of a board game based upon the traits that we fed them. To provide additional information that might aid the learners, we could incorporate the “category” attribute from the raw data that was excised in our preprocessing. We would have included it, except that doing so would have ballooned our number of attributes considerably, as it would have required making each categorical value into its own boolean attribute, as we did for the “mechanics” attribute. With the addition of genre and theme information, the classifiers might be able to come to more reliable conclusions. Also, we could consider running the data through a random forest learner. The decision trees proved slightly more effective than the ANNs at classifying the board games, so using random forests to further refine the results might improve the classification rate. Finally, we could split the quantiles into more precise ranges. It could be that our quantiles are too broad (each encompassing 20% of all examples), e.g. there could be\n",
    "particular qualities that are shared by games in the 75%-85% range that make it difficult to assign them to the highest or the second highest quantile. Making the quantiles more precise would reduce this problem at the risk of making the classification problem harder on average."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
