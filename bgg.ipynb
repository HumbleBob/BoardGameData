{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  jQuery(document).ready(function($) {  \n",
    "  \n",
    "  $(window).on('load', function(){\n",
    "    $('#preloader').fadeOut('slow',function(){$(this).remove();});\n",
    "  });\n",
    "  \n",
    "  });\n",
    "</script>\n",
    "\n",
    "<style type=\"text/css\">\n",
    "  div#preloader { position: fixed; \n",
    "      left: 0; \n",
    "      top: 0; \n",
    "      z-index: 999; \n",
    "      width: 100%; \n",
    "      height: 100%; \n",
    "      overflow: visible; \n",
    "      background: #fff url('http://preloaders.net/preloaders/720/Moving%20line.gif') no-repeat center center; \n",
    "  }\n",
    "\n",
    "</style>\n",
    "\n",
    "<div id=\"preloader\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  } \n",
    "  \n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false; \n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's in a Game? Analyzing Trends in Popular Board Games\n",
    "### Robert Choi, Andrew Walter-McNeill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When designing a product, predictive classification models can be used to identify what characteristics separate previously successful iterations of the product from less successful iterations. In some cases, the predictive power of these models may be strong enough to warrant incorporating their feedback into the design process. Board game design is an industry in the midst of a renaissance, with sales numbers and the variety of games in the market increasing year after year. Given that these games incorporate a variety of mechanics, genres, and intellectual properties, we were interested in the possibility that certain combinations of these attributes could predispose a game to critical and commercial success. Thankfully, the advent of BoardGameGeek.com's user-maintained database of board games has made such analyses possible. In this report, we will analyze the results of a predictive classification of a game’s quality based on its mechanics, complexity, and playtime. Additionally, we will compare the predictive abilities of decision trees and artificial neural networks when applied to this dataset using a variety of parameters. Read below for a description of our data, followed by an analysis of our decision tree results.  Continue reading for methodology and comparison of our models, along with the Python code used to generate these models. (Click the “Show Code” button in the top left-hand corner to reveal code.)  It should be noted that code used to create our models will run upon opening this notebook; the models and their results displayed below can and will be slightly different than the static results discussed in our analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we examined is a record of the BoardGameGeek.com “Board Game Rank” list compiled in March 2017. The list comprises 90,444 board games, about 5,000 of which are sufficiently well-known to be assigned a rating by the website’s hobbyist community—our dataset is a list of these 5,000 games. Each entry in the list includes the game’s name, publisher, author, date of publication, and “at-a-glance” information about the game’s content, such as its genre, mechanics, average play length, and degree of complexity. Our dataset can be used to investigate whether or not board games possessing particular qualities tend to be rated more highly by the website’s users, and it is remarkably robust in that it is vast, possesses no missing entries, and has perfect internal consistency (e.g. the mechanics classifications are assigned from a pre-made list, rather than assigned ad hoc). A full list of the attributes from the data that were considered by our learners is as follows:\n",
    "##### min_players \n",
    "The minimum number of players required to play a game. Typically 1-12, strictly integers. In our experience, 2-4 player games tend to rank most highly.\n",
    "##### max_players \n",
    "The maximum number of players allowed to play a game. Typically 2-12, strictly integers.\n",
    "##### avg_time\n",
    "The average number of minutes required to complete a game. Typically 15-240, strictly integers.\n",
    "##### min_time\n",
    "If, on a game's box, a range of times is printed for game length, then this attribute is the low end of that range. Typically 15-180, strictly integers.\n",
    "##### max_time\n",
    "If, on a game's box, a range of times is printed for game length, then this attribute is the high end of the range. Typically 30-240, strictly integers.\n",
    "##### age\n",
    "The guideline age printed on a game's box that recommends the minimum age of a player that can be expected to enjoy play. Typically 8-15, strictly integers.\n",
    "##### mechanic\n",
    "A list of categorical values describing the sorts of game mechanicsthat are included in a game's play. Examples include, \"Trading\", \"Action Point Allowance System\", \"Co-operative Play\", etc.\n",
    "##### weight\n",
    "A decimal value on a 5-point scale indicating how complex a game is, based on a number of factors, such as length of the game, length of time it takes to learn the game, complexity of the rule book, simpicity of design, length of time spent planning turn actions, and the amount of luck involved in play. Values range from 1-5, where 1 is \"light\" and 5 is \"heavy\".\n",
    "##### geek_rating\n",
    "The average of all user submitted ratings for a game with Bayesian Averaging. That is, the average, with additional dummy votes factored in. The effect of Bayesian Averaging is to favor games that have received many user votes over games with fewer user votes; generally, a game's rating will become more accurate as more votes accrue. This is our main class attribute, used to represent a game’s overall quality. Although we will use this attribute to approximate a game's quality, it is worth noting that game quality is a subjective measure, and determining quality by a vote is an imperfect method. For one thing, the Board Game Geek community is a small subset of the broader, board-game-playing public, with a particular set of preferences and priorities. Furthermore, as a result of the Bayesian Averaging, games with a larger player base are able to earn higher scores than games with a smaller audience, as the website's users outweigh the effect of the dummy votes. Possible ratings are from 1-10, but, in practice, values range from 5-9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "The raw dataset has two main problems with it: it contains many categorical attributes, and its class attribute is continuous. There were no missing values. In order to deal with the first issue, we limited the scope of this project to the categorical attribute we felt contained data most relevant to our class attribute (mechanic) and turned it into 52 binary attributes using a variation on one-hot encoding. We then removed the other categorical attributes. To address the second issue, we turned the continuous class attribute, geek_rating, into five quantiles; quantile 0 is populated by the 20% of games with the lowest geek_ratings, quantile 4 is populated by the 20% of games with the highest geek_ratings, etc. Our classification models will attempt to assign a game to one of these five quantiles. This preprocessing left us with 4999 examples, 61 attributes, and one class attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     26,
     117,
     143,
     174,
     184,
     205
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree                               # to do DTs\n",
    "from sklearn.datasets import fetch_mldata              # to import data\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier       # MLP: multi-layer perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#in order to directly edit individual cells in a pandas dataframe, this command\n",
    "#must be issued\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/HumbleBob/BoardGameData/master/original_data.csv'\n",
    "\n",
    "#genfromtxt reads data into a ndarray, chose this because loadtxt was not \n",
    "#working. Forces type string for each element so that all data can be \n",
    "#interpereted (specifying none defaults to ASCII byte codes because of the\n",
    "#strange encoding of the file). Selects important columns\n",
    "\n",
    "def transformData(raw_data):\n",
    "\n",
    "    cols = raw_data[0]\n",
    "    numpy_data = raw_data[1:]\n",
    "\n",
    "\n",
    "    #CREATES DATAFRAME\n",
    "    #transforms ndarray into pandas dataframe with headers specified by the first\n",
    "    #row\n",
    "    data = pd.DataFrame(data=numpy_data, columns=cols)\n",
    "    #specifies which attributes should be numeric (as they were all read in as str)\n",
    "    numeric_columns = ['min_players', 'max_players', 'avg_time', 'min_time', \n",
    "                       'max_time', 'geek_rating', 'num_votes', 'age', 'weight']\n",
    "    #translates specified columns into floats\n",
    "    data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric)\n",
    "\n",
    "    #creates quantiles for class attribute\n",
    "    geek_quantiles = pd.qcut(data['geek_rating'], 5, labels=False)\n",
    "    data['geek_quantiles'] = geek_quantiles\n",
    "\n",
    "\n",
    "    #PROCESSES CATEGORICAL ATTRIBUTE\n",
    "    #first creates empty list to store all categories\n",
    "    mechanics = []\n",
    "    #iterates over all attribute values (currently in str form)\n",
    "    for string_mechs in data['mechanic']:\n",
    "        #turns str into list of mechanics (find/replaced commas w '*' in excel)\n",
    "        list_mechs = string_mechs.split('*')\n",
    "        #gets rid of superfluous spaces and adds mechanic to master list if not\n",
    "        #already there\n",
    "        for mech in list_mechs:\n",
    "            if mech[0] == ' ':\n",
    "                clean_mech = mech[1:]\n",
    "                if clean_mech not in mechanics:\n",
    "                    mechanics.append(clean_mech)\n",
    "            elif mech not in mechanics:\n",
    "                mechanics.append(mech)\n",
    "\n",
    "    #creates new attribute to store each example's mechanics in list form\n",
    "    data['mechanic_list'] = 0\n",
    "\n",
    "    #iterates over index of DataFrame\n",
    "    for index in range(4999):\n",
    "        #gets rid of superfluous spaces again\n",
    "        list_mechs = []\n",
    "        dirty_mechs = data['mechanic'][index].split('*')\n",
    "        for mech in dirty_mechs:\n",
    "            if mech[0] == ' ':\n",
    "                clean_mech = mech[1:]\n",
    "                list_mechs.append(clean_mech)\n",
    "            else: \n",
    "                list_mechs.append(mech)\n",
    "        #for each example, defines newly created 'mechanic list' attribute to be\n",
    "        #a list of its mechanics\n",
    "        data['mechanic_list'][index] = list_mechs\n",
    "\n",
    "    #creates 43 new binary attributes, one for each mechanic\n",
    "    for mechanic in mechanics:\n",
    "        data[mechanic] = 0\n",
    "\n",
    "    #defines each binary appropriately: if the attribute appears in the an example's\n",
    "    #list of mechanics, defined as 1. Otherwise, 0.\n",
    "    for index in range(4999):\n",
    "        for mechanic in mechanics:\n",
    "            if mechanic in data['mechanic_list'][index]:\n",
    "                data[mechanic][index] = 1\n",
    "\n",
    "    #removes unneccessary attributes\n",
    "    del data['mechanic_list']\n",
    "    del data['mechanic']\n",
    "    del data['geek_rating']\n",
    "    del data['num_votes']\n",
    "    \n",
    "    #data = data.iloc[:,1:]\n",
    "    \n",
    "    #removes the headers from the rest of the data\n",
    "    headers = data.iloc[0,:]\n",
    "    #str_data = data.iloc[1:,:]\n",
    "    #turns the data from floats into strings\n",
    "    data = data.astype(float)\n",
    "    #separates the data from the class attributes\n",
    "    X = data.iloc[:,np.arange(len(headers)) != 7]\n",
    "    y = data.iloc[:,7]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "raw_data = np.genfromtxt(url, delimiter = ',', dtype=str, \n",
    "                        usecols=(3,4,5,6,7,10,11,13,14,17))\n",
    "X, y = transformData(raw_data)\n",
    "\n",
    "\n",
    "def confusionMatrix(X, y, learning, splitSize=.2, lbls=None):\n",
    "    '''Create a confusion matrix and report the matrix and the score\n",
    "    for one training/test split, given X, y, and a learning model as \n",
    "    an input. Allow the size of the split to be given as a parameter as well.\n",
    "    '''\n",
    "    # Make one test training split\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,\n",
    "                                                    test_size=splitSize,\n",
    "                                                    random_state=10)\n",
    "    model = learning.fit(Xtrain, ytrain)\n",
    "    y_model = model.predict(Xtest)\n",
    "    \n",
    "    score = accuracy_score(ytest, y_model)\n",
    "    # if specific labels are specified, implement those labels in the\n",
    "    # creation of the confusion matrix; otherwise, don't bother.\n",
    "    if(lbls==None):\n",
    "        mat = confusion_matrix(ytest, y_model)\n",
    "    else:\n",
    "        mat = confusion_matrix(ytest,\n",
    "                               y_model,\n",
    "                               labels=lbls)\n",
    "    print(\"Score and Confusion Matrix to illustrate problem complexity\")\n",
    "    print(\"Score: \", score)\n",
    "    print(\"Confusion Matrix:\\n\", mat)\n",
    "    print()\n",
    "\n",
    "def fiveFoldCVs(X, y, models):\n",
    "    '''Performs five five fold cross validations given X, y, and a \n",
    "    learning model\n",
    "    '''\n",
    "    cvs = []\n",
    "    \n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # For each model passed to the function, perform a 5foldcv and\n",
    "    # add it to the array to be returned\n",
    "    for i in range(len(models)):\n",
    "        score = cross_val_score(models[i], X, y, cv=cv)\n",
    "        cvs.append(score)\n",
    "    return cvs\n",
    "\n",
    "def getPValues(cvs):\n",
    "    '''Get the p-values for each pair of models, having been passed the \n",
    "    cv results\n",
    "    '''\n",
    "    # Basically, this offset variable and for loops exist so that every pair\n",
    "    # of models will have its p-value computed with no repeats\n",
    "    offset = 0\n",
    "    for i in range(len(cvs)):\n",
    "        for j in range(offset, len(cvs)):\n",
    "            if (i != j):\n",
    "                # p-values are computed and printed here\n",
    "                pair = stats.ttest_rel(cvs[i], cvs[j])\n",
    "                reportStr = \"The p-value for \" + str(i+1) + \",\" + str(j+1) + \" is \" + str(pair[1]) + \"\\n\"\n",
    "                print(reportStr)\n",
    "        offset += 1\n",
    "\n",
    "def getAverages(a):\n",
    "    '''get the averages of the input, an array of arrays (result of \n",
    "    cross validations)\n",
    "    '''\n",
    "    result = []\n",
    "    for i in a:\n",
    "        result.append(i.mean())\n",
    "    # return an array of averages corresponding to each array in the input\n",
    "    return result\n",
    "\n",
    "def getIntervals(a):\n",
    "    '''get the confidence intervals for an array of arrays \n",
    "    (result of cross validations)\n",
    "    '''\n",
    "    result = []\n",
    "    # set this mysterious value\n",
    "    z_critical = stats.norm.ppf(q = 0.975)\n",
    "    # get the averages to work with\n",
    "    means = getAverages(a)\n",
    "\n",
    "    # for each array\n",
    "    for i in range(len(a)):\n",
    "        # determine the standard deviation\n",
    "        standard_deviation = a[i].std()\n",
    "        # to calculate the margin of error\n",
    "        margin_error = (z_critical * standard_deviation) / (math.sqrt(len(a[i])))\n",
    "        \n",
    "        # so as to calculate the confidence interval and add it to the result\n",
    "        result.append([means[i] - margin_error, means[i] + margin_error])\n",
    "    return result\n",
    "\n",
    "def cIPlot(data, ymin=0.3):\n",
    "    '''Create a confidence interval graph and save it as a file. The\n",
    "    results of the cross validations are passed as a parameter, as is\n",
    "    the name of the file to be saved.\n",
    "    '''\n",
    "    # Get the averages and confidence intervals for the graph\n",
    "    sample_means = getAverages(data)\n",
    "    intervals = getIntervals(data)\n",
    "\n",
    "    # Set size\n",
    "    plt.figure(figsize=(9,9))\n",
    "\n",
    "    xvals = np.arange(5, 30, 5) # This is important given the size of the data\n",
    "                                # somehow.\n",
    "    yerrors = [(top-bot)/2 for top,bot in intervals]\n",
    "\n",
    "    plt.errorbar(x=xvals,\n",
    "                 y=sample_means,\n",
    "                 yerr=yerrors,\n",
    "                 fmt='D')\n",
    "\n",
    "    # Standardize the graph scale\n",
    "    plt.axis(xmin=0,xmax=30,ymin=ymin,ymax=1.05)\n",
    "    # Apply labels\n",
    "    labels = [\"v1\", \"v2\", \"v3\", \"v4\", \"v5\"]\n",
    "    plt.xticks(xvals,labels)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of Decision Tree Results\n",
    "![title](https://raw.githubusercontent.com/HumbleBob/BoardGameData/master/Figure1.png)\n",
    "\n",
    "The precise classification accuracy of all of the decision trees created in this project hovered slightly above 25%. While 25% accuracy can hardly be called reliable, this measure presents a slightly pessimistic evaluation of the trees’ performance.  Because the geek_rating class attribute is an ordinal, rather than a nominal value, classification techniques such as decision trees tend to suffer because they cannot take into account the relations between class values.  To illustrate, it may be considered better for a classifier to mistakenly place a game from the 5th quantile into the 4th quantile rather than the 1st quantile, but classifiers are typically not able to draw this connection.  Given that this is the case, it is encouraging that the confusion matrix for our decision trees show that classifications are more concentrated closest to the matrix’s diagonal line, indicating that games are likely to be classified into a quantile at least close to the one in which they belong.  The decision tree with the highest classification accuracy was tree v5, pictured above, which included a prepruning parameter limiting the number of its leaves to 50.  We will parse the branches of this tree for potential trends. Considering the difficulty posed by this ordinal classification task, we will sometimes group the quantiles together, the top two quantiles to represent “good” games, and the bottom three quantiles to represent mediocre or lacking games. This turns the classification task into a binary one, which allows us a foundation for speaking about the observed trends.\n",
    "![title](https://raw.githubusercontent.com/HumbleBob/BoardGameData/master/Figure2.png)\n",
    "\n",
    "The first split separates the 558 games that have a “Hex-and-Counter” mechanic from those that don’t. Games that have a “Hex-and-Counter” mechanic belong predominantly to the lower quantiles of geek_rating: 60% belong to the bottom two quantiles, and 83% belong to the bottom three quantiles. What can explain this broad trend in reception? The “Hex-and-Counter” mechanic belongs almost exclusively to classic, hex-based wargames, and interest in these games is concentrated in a niche sub-community of the board game enthusiast crowd. Despite this, wargames depicting the events of such conflicts as the Napoleonic Wars or World War II are extremely common; of the mechanics included in our dataset, “Hex-and-Counter” is the 4th most prevalent, exceeded only by extremely common mechanics such as “Dice Rolling” and “Hand Management”. These wargames are so widespread in part because their fans are so hardcore—the story of player-turned-amateur-designer is commonplace in this community, so the market is saturated with would-be designers’ first attempts and failures. Furthermore, wargames will frequently focus their depiction of historical battles on individual skirmishes or battles; in addition to games that broadly depict World War II, for instance, there will be multitudes more that depict Normandy, Iwo Jima, or Stalingrad. As a result, there are more of these complex, lengthy wargames than an individual could possibly play in a lifetime; this fact, combined with the relatively small audience for these games, means that only a few cream-of-the-crop entries can garner enough attention and votes to outweigh the dummy votes of geek_rating’s Bayesian Averaging system. Moreover, as mentioned earlier, many of these wargames are simply not that enjoyable to play—they are frequently antiquated, overly complex, or amateurish in design.\n",
    "![title](https://raw.githubusercontent.com/HumbleBob/BoardGameData/master/Figure3.png)\n",
    "\n",
    "On the other, left side of the tree, there is a series of upper-level splits that are based on one attribute: weight, which is a numerical approximation of the complexity of a game. These splits are in order of decreasing weight; the games in the branch created by the first split are the most complex, and the games in the branch left over by the last split are the least complex. With each of these splits, the proportion of games in the two highest geek_rating quantiles decreases. Before any weight comparisons, 43% of games belong to the two highest quantiles; after removing games with weight greater than 2.6719, only 37% belong to these quantiles; after removing games with weight greater than 1.6676, only 27%; after removing games with weight greater than 1.0294, only 7%. That games with above average weight possess better geek_rating on average is indicative of Board Game Geek’s hobbyist audience. The voters on the website are enthusiasts; they are likely experienced gamers who are looking for games to challenge and stimulate them. This preference for complex games is borne out by the data: the weight values of the 5 highest ranked games are 2.8067, 4.2953, 3.528, 3.9374, and 3.56, respectively. For the average person, the prospect of a 4 hour game of Through the Ages: A New Story of Civilization might be nightmarish; for a gaming enthusiast, it is exciting. On the other hand, Monopoly, one of the most popular, mainstream board games of all time, with its weight of 1.68, is ranked by the Board Game Geek community as the 14,116th best game available, too low quality to even be considered in our dataset of 5,000 entries. The simplicity of choice and the involvement of luck common to games of low weight tends to bore experienced gamers.\n",
    "![title](https://raw.githubusercontent.com/HumbleBob/BoardGameData/master/Figure4.png)\n",
    "\n",
    "Limiting our attention to the branch with the greatest collective weight value, we can note that games from the two highest quantiles already constitute 58% of the entries in this branch. The branch comprises 17 terminating leaves, which classify games according to the following table:\n",
    "\n",
    "| Quantile Classification | Number of leaves        |\n",
    "|-------------------------|-------------------------|\n",
    "| Quantile 5 (highest)    | 11                      |\n",
    "| Quantile 4              | 2                       |\n",
    "| Quantile 3              | 1                       |\n",
    "| Quantile 2              | 0                       |\n",
    "| Quantile 1 (lowest)     | 3                       |\n",
    "\n",
    "Two of these 17 leaves have a fairly high number of samples and extremely high entropy. Of these leaves, we can speculate that further splits would have been made, were it not for the pre-pruning measure limiting the tree to 50 total terminating leaves. 278 of the 513 games belonging to the bottom three quantiles in this branch are sorted into these two leaves; due to the pre-pruning, it seems that the classifier largely “gave up” on finding what, if any, factors reliably except games from the “more complex is better” trend.\n",
    "![title](https://raw.githubusercontent.com/HumbleBob/BoardGameData/master/Figure5.png)\n",
    "\n",
    "There are a few exceptions where the classifier identified factors that tend to cause a more complex game to be poorly received. The 3 terminating leaves that classify games into the lowest quantile have relatively low entropy, meaning that their classifications were fairly reliable. The defining characteristics of these leaves may reveal somewhat humorous insights into what games may be poorly received. The first of these leaves consists of games that meet the initial complexity bar of weight greater than 2.6719, but have a maximum expected playtime of less than 25 minutes. In a community where an expected playtime of 2.5+ hours is no obstacle to critical acclaim, perhaps there is an expectation that the effort required to learn a complicated game will be rewarded with a sufficiently long playtime to experience the depth that gameplay complexity can enable. Continuing this pattern, the second of these lowest quantile leaves consists of games with even greater complexity (weight greater than 3.1204) and maximum expected playtime of less than 42.5 minutes. As a game becomes more complex, the expectations for its length may become greater as well. Finally, the last of the lowest quantile leaves suggests that games of sufficient complexity (weight greater than 2.6719) with an average playtime of greater than 97.5 minutes and a recommended age of less than 4 years tend to be poorly received. It can be inferred that games of this length and complexity do not usually go over well with their intended audience of 3-year-olds and their parents.\n",
    "![title](https://raw.githubusercontent.com/HumbleBob/BoardGameData/master/Figure6.png)\n",
    "\n",
    "In classifying games from the higher quantiles, bearing in mind the difficulties of ordinal classification, the decision tree sees significantly more success; for 10 of the 11 leaves that classify games as belonging to the highest quantile, a majority of the games from our dataset that are sorted into these leaves belong to the two highest quantiles. The defining qualities of these high quantile leaves tend to be very simple: do they meet the initial bar for gameplay complexity and do they possess some popular mechanic—game systems such as “Hand Management,” “Dice Rolling,” “Worker Placement,” or “Set Collection” that can be called tried-and-true. The enduring popularity of these mechanics is attributable to the ways in which they tend to promote characteristics of great game design: specifically, a satisfying balance of luck and skill, combined with a series of meaningful choices. We can take “Hand Management” as a textbook example of both. \n",
    "\n",
    "Firstly, managing a set of cards, presumably drawn randomly from a shuffled deck, necessarily introduces an element of luck to the outcome of the game; implemented smartly, however, this luck factor is just another facet that defines skillful play of the game. For instance, if deck building is a part of the game’s strategy, then skillful play will involve designing a deck that maximizes the odds of drawing effective cards; in any case, skillful play will certainly necessitate choosing and executing the most effective strategy within the constraints of the player’s hand of cards. The involvement of luck can also make a game more enjoyable in and of itself—luck can increase the game’s tension as players await an uncertain outcome, or it can give a less experienced player a shot at victory. Luck also has the effect of modifying a game’s replayability; that each play of a game will begin and proceed slightly differently rewards players for playing again and experiencing the many strategies and outcomes that the game has to offer. These benefits are all to a game’s advantage, so long as the influence of luck does not overshadow that of skill. \n",
    "\n",
    "Secondly, a hand of cards implies a set of choices available to the player. What counts as a meaningful choice? One way of drawing this distinction is to ask “will this choice impact the outcome of the game?” and “is the best option available an obvious one?” If the answer to the first question is “no,” then the choice has no consequences; if the answer to the second question is “yes,” then the obviously suboptimal options are not authentic choices. The choices offered by a hand of cards can be impactful and challenging; one must determine not only which cards to play, but also at what point in the game and which cards must be discarded. This multitude of options decreases the likelihood that the most effective strategy will be obvious to the players. Each of the mechanics that is the characteristic attribute of one of the high quantile leaves in this branch has a demonstrated capacity for a satisfying skill/luck balance and meaningful choices, and the high weight rating for the games in this branch suggests that these games are complex and well-designed enough to leverage this capacity.\n",
    "\n",
    "If the intent in analyzing this decision tree is to determine the optimal route to designing a successful board game, then the simple trends demonstrated by these highly-ranked games are hardly helpful. No set of mechanics will lead to a game’s being automatically successful; rather, the difficulty is in designing a game sufficiently complex and stimulating that familiar mechanics don’t lose their intrigue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "Many compromises needed to be made during this project to accommodate the continuous class attribute to the classification task.  The geek_rating attribute needed to be converted to quantiles, which give a less precise impression of a game’s relative quality, but even this process did not prevent our classifiers from struggling with what was now an ordinal classification task.  Given that this difficulty persisted throughout the project, future efforts with this dataset could be based upon regression models, rather than classification models.  \n",
    "\n",
    "Leaving this major change aside, other tweaks could be made to our methods to expand and clarify our results.  First, our dataset could be expanded.  As of this writing, the Board Game Geek rating list extends past 5,000 entries, into the 10,000's and beyond.  Including all of the ranked games on the website would provide our classifiers with something they’ve yet to encounter: a recognizably bad game.  As of now, all of the entries in our dataset have a geek_rating of at least 5, which, in theory, represents a game of middling quality.  Having more variance in quality throughout the dataset might permit more trends to be recognized by the learners.  \n",
    "\n",
    "Second, we could include more of the features included as a part of our dataset.  While mechanics, we felt, were the most pertinent to the purpose of our project, other attributes, such as “category,\" which characterizes the setting or genre of the game, could also be included, as this attribute might provide more meaningful information for our learners to work with.\n",
    "\n",
    "Third, we could conduct parameter sweeps on each of our models to maximize their effectiveness; we could also transition from prepruning our decision trees to a postpruning process.  Each of the models we created in this project used a variety of parameters in an attempt to capture the most effective version possible, but using an automated process of parameter sweeping could identify new, more effective combinations.  Furthermore, because our prepruned decision trees contained leaves with a high number of samples and high entropy, we can expect that more splits and better classification are possible for the samples in these leaves.  However, that our prepruned trees performed better than our unpruned, control tree suggests that the unhampered tree was prone to overfitting.  Some sort of postpruning would be necessary to ensure the tree can generalize beyond the learning data.\n",
    "\n",
    "Finally, if work with classifiers is to continue, the size of the quantiles used could be modified to give a better idea of the accuracy achieved by our models.  Given that our analysis tended treat the problem as though it were a binary classification task, it may be helpful to use larger quantiles in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "Decision trees work by separating a data set into different nested “nodes” of data with the same value for a given attribute. They are a simple, elegant way of dealing with samples with many attributes, are robust to noisy data, can be easily understood by humans, and usually yield results that are appreciably better than random guessing. Artificial neural nets work by passing the attribute values of a given sample through a series of nodes until a single output signal is produced, much in the same way as neurons pass signals along in the brain. While ANNs are much more difficult for humans to understand than decision trees, they are uniquely well equipped to handle Big Data (incremental training, dynamic data sets etc) and are thus an increasingly intriguing option. They are also robust to noise and can easily deal with complex attributes and data sets.\n",
    "\n",
    "For our five decision tree runs, we chose to manipulate the criterion used to make splits, the maximum depth of the tree, the minimum samples required to form a leaf, and the maximum number of leaf nodes. The logic for each manipulation is as follows (parameters listed):\n",
    "##### v1: (criterion = entropy)\n",
    "The Control model\n",
    "##### v2: (criterion = gini)\n",
    "The ‘entropy’ algorithm utilizes information gain to make splits .  Information gain is a measure of exactly what it sounds like: the information gained from a particular split. A 50/50 split divides the data no better than random distribution, and thus provides the least amount of information possible, whereas a 100/0 split provides the most information possible (if a sample has that combination of attribute values, its class value is known). The ‘entropy’ algorithm calculates the potential information gain for all possible splits, and chose the split that results in the highest IG. The ‘gini’ algorithm operates by determining what the probability of a improper classification is for each sample in a given node if said classification was done randomly within the proportions of attribute value distribution in the given node. This, combined with the control is an exploration of the relative effectiveness of the two splitting algorithms.\n",
    "##### v3: (criterion = ‘entropy’, max_depth = 2)\n",
    "One of the ways to combat overfitting in decision trees is to limit the depth of the tree. By limiting how many layers down a tree can build, one also limits the complexity of the tree, and thus lowers the likelihood that it will conform too specifically to a given data set. We chose a value of 2 because 2 is the most restrictive possible setting; by making the change to the classifier as dramatic as possible, we hope to most clearly illuminate any trends that might result from analogous changes. This is perhaps the most basic approach to combatting overfitting, as it does not take into account more informative measures of complexity, like the size of the nodes in the tree or the number of nodes in the tree.\n",
    "##### v4: (criterion = ‘entropy, min_samples_leaf = 25)\n",
    "Another way to combat overfitting in decision trees is to define a minimum number of samples each leaf must have. This constraint, not unlike the max_depth constraint, limits the complexity of the decision tree by forcing it to stop splitting nodes at an earlier point than it otherwise would have (resulting in less overall nodes). After brief visual analysis of the unconstrained decision tree, we noticed that most of the smaller leaves contained around 10 or 15 examples. We thought doubling this number would give us an appreciable reduction in complexity without sacrificing too much accuracy, and so chose 25.\n",
    "##### v5: (criterion = ‘entropy’, max_leaf_nodes = 50)\n",
    "One more way to combat overfitting is to limit the number of leaves the tree can have. This essentially accomplishes the same thing as the previous two parameter changes, limiting the complexity of the tree, but it does so in a different way. After brief visual analysis, we noticed that the unconstrained tree had a little less than 100 leaves. So as to be consistent with our factor-of-two approach in the last classifier and hopefully accomplish the same thing, we decided to half the unconstrained value and got 50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Trees\n",
      "\n",
      "Score and Confusion Matrix to illustrate problem complexity\n",
      "Score:  0.262\n",
      "Confusion Matrix:\n",
      " [[56 43 52 39 17]\n",
      " [45 30 50 45 24]\n",
      " [44 40 45 33 34]\n",
      " [40 27 44 50 41]\n",
      " [17 29 35 39 81]]\n",
      "\n",
      "Confidence Intervals:\n",
      "v1:  [ 0.273  0.252  0.268  0.263  0.252]\n",
      "v2:  [ 0.263  0.269  0.253  0.252  0.246]\n",
      "v3:  [ 0.251  0.254  0.255  0.256  0.262]\n",
      "v4:  [ 0.279  0.28   0.278  0.277  0.275]\n",
      "v5:  [ 0.275  0.285  0.293  0.275  0.283]\n",
      "\n",
      "The p-value for 1,2 is 0.42866508991\n",
      "\n",
      "The p-value for 1,3 is 0.343909459169\n",
      "\n",
      "The p-value for 1,4 is 0.0165200132837\n",
      "\n",
      "The p-value for 1,5 is 0.0253737756901\n",
      "\n",
      "The p-value for 2,3 is 0.868273624092\n",
      "\n",
      "The p-value for 2,4 is 0.00309580721739\n",
      "\n",
      "The p-value for 2,5 is 0.0100805299978\n",
      "\n",
      "The p-value for 3,4 is 0.00102675240209\n",
      "\n",
      "The p-value for 3,5 is 0.00161076662681\n",
      "\n",
      "The p-value for 4,5 is 0.270487852341\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAIMCAYAAABVH87kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWYYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ee151d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_decision_trees(X, y):\n",
    "    '''Function to run all parts of DT creation and testing\n",
    "    '''\n",
    "    print(\"Decision Trees\\n\")\n",
    "\n",
    "    # Make a list of trees\n",
    "    trees = []\n",
    "    #creates 5 different decision tree classifiers with different parameters and \n",
    "    #fits the data to them. See paper for parameter explanations\n",
    "    dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "    trees.append(dt)\n",
    "    dt = tree.DecisionTreeClassifier(criterion='gini')\n",
    "    trees.append(dt)\n",
    "    dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth = 2)\n",
    "    trees.append(dt)\n",
    "    dt = tree.DecisionTreeClassifier(criterion='entropy', min_samples_leaf = 25)\n",
    "    trees.append(dt)\n",
    "    dt = tree.DecisionTreeClassifier(criterion='entropy', max_leaf_nodes = 50)\n",
    "    trees.append(dt)\n",
    "    \n",
    "    return trees\n",
    "\n",
    "trees = create_decision_trees(X, y)\n",
    "\n",
    "confusionMatrix(X, y, trees[0])\n",
    "\n",
    "# Compute cross validations for each model and print them\n",
    "cvs = fiveFoldCVs(X, y, trees)\n",
    "print(\"Confidence Intervals:\")\n",
    "for i in range(len(cvs)):\n",
    "    print(\"v\" + str(i+1) + \": \", cvs[i])\n",
    "print()\n",
    "\n",
    "# Compute p-values\n",
    "getPValues(cvs)\n",
    "\n",
    "# Create file for confidence intervals\n",
    "cIPlot(cvs, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANNs\n",
    "After researching the neural net parameters, learnrate stood out to us as the most intriguing. We thus focused our trials on manipulating learnrate, providing two controls. We also explored more basic changes to the number of iterations and the structure of the neural net. The logic for each manipulation is as follows (parameters listed):\n",
    "##### v1: (X, y, numIter=500)\n",
    "Control. Default number of iterations has been raised to increase general\n",
    "accuracy.\n",
    "##### v2: (X, y, numIter=500, numNodes = 15, numLayers = 3)\n",
    "For this manipulation, we chose to set the number of nodes to 15 and the number of hidden layers to 3. For most problems, a single hidden layer is sufficient, as the ANN weighting algorithms are complex enough that they do not need much to work with (see following versions), but we wanted to see what would happen when we set the number of layers at three, working with just the default other parameters. Theoretically, the increased complexity might increase accuracy, but it will also increase runtime.\n",
    "##### v3: (X, y, numIter=500, solve = ‘sgd’)\n",
    "Control (solve method). The default solver is ‘adam,’ which, according to\n",
    "sklearn’s website, is “works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score.” From brief research on stack exchange, the ‘adam’ solver is actually an adaptation of the more universal stochastic gradient descent (sgd); therefore, we thought it would be interesting to compare sklearn’s default solver to the primary default solver in the field. This exploration also provides the control for our investigations of learnrate, which all require the sgd solve method.\n",
    "##### v4: (X, y, numIter=500, learnRate = 'adaptive', solve = 'sgd')\n",
    "The learnrate of an artificial neural net determines how quickly the ANN deserts old generalizations of the dataset for new ones. For example, suppose the ANN encounters 10 board game samples that have average playtimes of 60 minutes and have a geek rating of 5 (the highest quantile). Then suppose it encountered an 11th board game that has an average playtime of 30 minutes, but also had a geek rating of 5. If the learnrate of the ANN is relatively high, it will assume that sometimes even games with playtimes lower than 60 minutes can be rated highly; if the learnrate is relatively low, it will assume that the 11th board game is an outlier, and that only board games with playtimes of 60 minutes can be rated highly. The ‘adaptive’ parameter keeps the learnrate of the ANN at a predetermined value throughout the course of training, unless the training loss stops decreasing. If training loss ever does begin to increase, the learning rate is divided by five. The ANN should get better and better at predicting as it trains, as it learns incrementally. If this does not happen, its training loss will increase, and it will begin to “learn faster.” Since a higher learning rate means longer train time, this is one way to find an reasonable learning rate for a given data set.\n",
    "##### v5: (X, y, numIter=500, learnRate = 'invscaling', solve = 'sgd')\n",
    "One other possible learnrate parameter is invscaling. Invscaling works by steadily\n",
    "decreasing the learning rate (roughly following exponential decay) over a time interval t. Because invscaling allows the ANN to learn very quickly initially and then more slowly as time goes on, it can provide runtime advantages without compromising accuracy  but only with static datasets . If the dataset it dynamic, and new examples are being constantly added, invscaling is a poor choice because it will not allow the ANN to update any of its initial beliefs as quickly as it should. Since our dataset is static, we should be able to appreciate an increase in runtime without a loss of accuracy by utilizing invscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     19
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNs\n",
      "\n",
      "Score and Confusion Matrix to illustrate problem complexity\n",
      "Score:  0.309\n",
      "Confusion Matrix:\n",
      " [[ 94  17  21  18  57]\n",
      " [ 77  10  18  23  66]\n",
      " [ 61  13  23  18  81]\n",
      " [ 46  12  14  22 108]\n",
      " [ 14   1  10  16 160]]\n",
      "\n",
      "Confidence Intervals:\n",
      "v1:  [ 0.294  0.28   0.313  0.318  0.317]\n",
      "v2:  [ 0.309  0.277  0.32   0.311  0.306]\n",
      "v3:  [ 0.204  0.193  0.187  0.18   0.184]\n",
      "v4:  [ 0.204  0.193  0.187  0.182  0.184]\n",
      "v5:  [ 0.204  0.193  0.187  0.22   0.221]\n",
      "\n",
      "The p-value for 1,2 is 0.968486746913\n",
      "\n",
      "The p-value for 1,3 is 0.000462134240533\n",
      "\n",
      "The p-value for 1,4 is 0.000434831908457\n",
      "\n",
      "The p-value for 1,5 is 0.000138037239823\n",
      "\n",
      "The p-value for 2,3 is 0.000234853420856\n",
      "\n",
      "The p-value for 2,4 is 0.000221591583612\n",
      "\n",
      "The p-value for 2,5 is 0.000404695283501\n",
      "\n",
      "The p-value for 3,4 is 0.3739009663\n",
      "\n",
      "The p-value for 3,5 is 0.178239554897\n",
      "\n",
      "The p-value for 4,5 is 0.177858401051\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAIMCAYAAABVH87kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ee2b8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def makeMLP(X, y, numNodes=10, numLayers=3, numIter=200, learnRate='constant', solve='adam'):\n",
    "    '''Make an ANN. Allow the number of nodes and the number of hidden layers\n",
    "    and the maximum number of iterations to be set as parameters. \n",
    "    Returns the ANN.'''\n",
    "    \n",
    "    # Create a hidden layer tuple based on the parameters passed to the\n",
    "    # function\n",
    "    hiddenLayers = [numNodes for i in range(numLayers)]\n",
    "    hiddenLayers = tuple(hiddenLayers)\n",
    "\n",
    "    # Initialize ANN classifier using hiddenLayers tuple to set sizes\n",
    "    ann = MLPClassifier(hidden_layer_sizes=hiddenLayers, \n",
    "                        activation='logistic',      # default is 'relu'\n",
    "                        max_iter = numIter,         # default is 200\n",
    "                        learning_rate = learnRate,\n",
    "                        solver = solve,\n",
    "                        random_state=0)             # seed\n",
    "    return ann\n",
    "\n",
    "def create_anns(X, y):\n",
    "    print(\"ANNs\\n\")\n",
    "    \n",
    "    # Make a list of ANNs, each with a number of hidden nodes and number\n",
    "    # of iterations specified in the lists \"hNodes\" and \"maxIters\" respectively\n",
    "    # passed as a parameters\n",
    "    anns = []\n",
    "\n",
    "    #creates 5 artificial neural net models with 5 different sets of parameters. \n",
    "    #see paper for parameter explanations.\n",
    "    ann = makeMLP(X, y, numIter = 500)\n",
    "    anns.append(ann)\n",
    "    ann = makeMLP(X, y, numIter = 500, numNodes = 15, numLayers = 3)\n",
    "    anns.append(ann)\n",
    "    ann = makeMLP(X, y, numIter = 500, solve = 'sgd')\n",
    "    anns.append(ann)\n",
    "    ann = makeMLP(X, y, numIter = 500, learnRate = 'adaptive', solve = 'sgd')\n",
    "    anns.append(ann)\n",
    "    ann = makeMLP(X, y, numIter = 500, learnRate = 'invscaling', solve = 'sgd')\n",
    "    anns.append(ann)\n",
    "    \n",
    "    return anns\n",
    "\n",
    "anns = create_anns(X, y)\n",
    "\n",
    "confusionMatrix(X, y, anns[0])\n",
    "\n",
    "# Compute cross validations for each model and print them\n",
    "cvs = fiveFoldCVs(X, y, anns)\n",
    "print(\"Confidence Intervals:\")\n",
    "for i in range(len(cvs)):\n",
    "    print(\"v\" + str(i+1) + \": \", cvs[i])\n",
    "print()\n",
    "\n",
    "# Compute p-values\n",
    "getPValues(cvs)\n",
    "\n",
    "# Create file for confidence intervals\n",
    "cIPlot(cvs, 0.1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The maximum classification accuracy for all of our models and cross validations was 0.315, from v1 of our ANNs. The minimum classification accuracy was 0.18 from v3 of our ANNs. All of our classifiers have accuracies falling between this range of 0.18-0.315. Given the p-values for our decision trees, the differences between v1, v2, and v3 were not statistically significant, and the differences between v4 and v5 were also not statistically significant. However, the differences between these two groups of trees were statistically significant. Among our decision trees, v4 and v5 performed the best, and their superior performance compared to v1, v2, and v3 is statistically significant. This suggests that the parameters implemented here to reduce overfitting were somewhat effective. Similarly, given the p-values for our ANNs, the differences between v1 and v2 were not statistically significant, and the differences between v3, v4, and v5 were also not statistically significant. However, the differences between these two groups of ANNs were statistically significant. Among our ANNs, v1 and v2 performed the best, and their superior performance compared to v3, v4, and v5 is statistically significant. This suggests that the default solver for sklearn’s MLPclassifier is superior compared to sgd when applied to this dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#999; background:#fff;\">\n",
    "Created with Jupyter, delivered by Fastly, rendered by Rackspace.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
